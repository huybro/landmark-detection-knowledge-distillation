{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e554b24",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-19T19:00:02.826747Z",
     "iopub.status.busy": "2024-11-19T19:00:02.826464Z",
     "iopub.status.idle": "2024-11-19T19:00:16.549787Z",
     "shell.execute_reply": "2024-11-19T19:00:16.548742Z"
    },
    "papermill": {
     "duration": 13.731018,
     "end_time": "2024-11-19T19:00:16.552030",
     "exception": false,
     "start_time": "2024-11-19T19:00:02.821012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\r\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-1tjfpmvj\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-1tjfpmvj\r\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting ftfy (from clip==1.0)\r\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2024.5.15)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.4.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.19.0)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.1.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.6.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\r\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: clip\r\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=bfa51393ed0a5820d7566e8601b2d0aee02032a3fc077b341f7680afbab57709\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-snvejyai/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\r\n",
      "Successfully built clip\r\n",
      "Installing collected packages: ftfy, clip\r\n",
      "Successfully installed clip-1.0 ftfy-6.3.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af2cbcc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:00:16.563053Z",
     "iopub.status.busy": "2024-11-19T19:00:16.562786Z",
     "iopub.status.idle": "2024-11-19T19:00:26.523996Z",
     "shell.execute_reply": "2024-11-19T19:00:26.522890Z"
    },
    "papermill": {
     "duration": 9.9687,
     "end_time": "2024-11-19T19:00:26.525921",
     "exception": false,
     "start_time": "2024-11-19T19:00:16.557221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /opt/conda/lib/python3.10/site-packages (1.4.17)\r\n",
      "Collecting albumentations\r\n",
      "  Downloading albumentations-1.4.21-py3-none-any.whl.metadata (31 kB)\r\n",
      "Requirement already satisfied: numpy>=1.24.4 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.14.1)\r\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from albumentations) (6.0.2)\r\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (2.9.2)\r\n",
      "Collecting albucore==0.0.20 (from albumentations)\r\n",
      "  Downloading albucore-0.0.20-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Requirement already satisfied: eval-type-backport in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.2.0)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /opt/conda/lib/python3.10/site-packages (from albumentations) (4.10.0.84)\r\n",
      "Collecting stringzilla>=3.10.4 (from albucore==0.0.20->albumentations)\r\n",
      "  Downloading stringzilla-3.10.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting simsimd>=5.9.2 (from albucore==0.0.20->albumentations)\r\n",
      "  Downloading simsimd-6.0.7-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (58 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\r\n",
      "Downloading albumentations-1.4.21-py3-none-any.whl (227 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.9/227.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading albucore-0.0.20-py3-none-any.whl (12 kB)\r\n",
      "Downloading simsimd-6.0.7-cp310-cp310-manylinux_2_28_x86_64.whl (606 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.4/606.4 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading stringzilla-3.10.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (291 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.7/291.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: stringzilla, simsimd, albucore, albumentations\r\n",
      "  Attempting uninstall: albucore\r\n",
      "    Found existing installation: albucore 0.0.17\r\n",
      "    Uninstalling albucore-0.0.17:\r\n",
      "      Successfully uninstalled albucore-0.0.17\r\n",
      "  Attempting uninstall: albumentations\r\n",
      "    Found existing installation: albumentations 1.4.17\r\n",
      "    Uninstalling albumentations-1.4.17:\r\n",
      "      Successfully uninstalled albumentations-1.4.17\r\n",
      "Successfully installed albucore-0.0.20 albumentations-1.4.21 simsimd-6.0.7 stringzilla-3.10.10\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271816c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:00:26.539673Z",
     "iopub.status.busy": "2024-11-19T19:00:26.539358Z",
     "iopub.status.idle": "2024-11-19T19:00:37.424315Z",
     "shell.execute_reply": "2024-11-19T19:00:37.423351Z"
    },
    "papermill": {
     "duration": 10.893546,
     "end_time": "2024-11-19T19:00:37.426437",
     "exception": false,
     "start_time": "2024-11-19T19:00:26.532891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import timm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import albumentations as A\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import clip\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4099ddd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:00:37.438633Z",
     "iopub.status.busy": "2024-11-19T19:00:37.438173Z",
     "iopub.status.idle": "2024-11-19T19:00:37.443182Z",
     "shell.execute_reply": "2024-11-19T19:00:37.442516Z"
    },
    "papermill": {
     "duration": 0.012822,
     "end_time": "2024-11-19T19:00:37.444871",
     "exception": false,
     "start_time": "2024-11-19T19:00:37.432049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "DEBUG = False\n",
    "N_WORKERS = os.cpu_count() \n",
    "USE_AMP = False # can change True if using T4 or newer than Ampere\n",
    "SEED = 42\n",
    "CLIP_MODEL_NAME = \"RN101\"\n",
    "IMG_SIZE = [224, 224]\n",
    "AUG_PROB = 0.2\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "if DEBUG:\n",
    "    EPOCHS = 3\n",
    "GRAD_ACC = 1\n",
    "N_FOLDS = 5\n",
    "MAX_GRAD_NORM = None\n",
    "EARLY_STOPPING_EPOCH = 3\n",
    "OUTPUT_DIR = f'clip_landmark_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf6a4ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:00:37.456362Z",
     "iopub.status.busy": "2024-11-19T19:00:37.456115Z",
     "iopub.status.idle": "2024-11-19T19:00:37.459835Z",
     "shell.execute_reply": "2024-11-19T19:00:37.459148Z"
    },
    "papermill": {
     "duration": 0.011157,
     "end_time": "2024-11-19T19:00:37.461367",
     "exception": false,
     "start_time": "2024-11-19T19:00:37.450210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93c5d91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:00:37.473049Z",
     "iopub.status.busy": "2024-11-19T19:00:37.472408Z",
     "iopub.status.idle": "2024-11-19T19:00:38.292118Z",
     "shell.execute_reply": "2024-11-19T19:00:38.291190Z"
    },
    "papermill": {
     "duration": 0.827342,
     "end_time": "2024-11-19T19:00:38.293895",
     "exception": false,
     "start_time": "2024-11-19T19:00:37.466553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>landmark_id</th>\n",
       "      <th>category</th>\n",
       "      <th>supercategory</th>\n",
       "      <th>hierarchical_label</th>\n",
       "      <th>natural_or_human_made</th>\n",
       "      <th>cleaned_supercategory</th>\n",
       "      <th>image_path</th>\n",
       "      <th>landmark_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73792c1235ff2625</td>\n",
       "      <td>33618</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Category:Don...</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>human-made</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>/kaggle/input/google-landmarks-v2-index-set-p3...</td>\n",
       "      <td>Dona i Ocell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a3751bcb2740c891</td>\n",
       "      <td>33618</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Category:Don...</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>human-made</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>/kaggle/input/google-landmarks-v2-index-set-p4...</td>\n",
       "      <td>Dona i Ocell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2f32b63b86113c22</td>\n",
       "      <td>33618</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Category:Don...</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>human-made</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>/kaggle/input/google-landmarks-v2-index-set-p1...</td>\n",
       "      <td>Dona i Ocell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>657c10997e4f399c</td>\n",
       "      <td>33618</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Category:Don...</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>human-made</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>/kaggle/input/google-landmarks-v2-index-set-p2...</td>\n",
       "      <td>Dona i Ocell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>029ba12e494a33ff</td>\n",
       "      <td>33618</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Category:Don...</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>human-made</td>\n",
       "      <td>sculpture</td>\n",
       "      <td>/kaggle/input/google-landmarks-v2-index-set-p1...</td>\n",
       "      <td>Dona i Ocell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  landmark_id  \\\n",
       "0  73792c1235ff2625        33618   \n",
       "1  a3751bcb2740c891        33618   \n",
       "2  2f32b63b86113c22        33618   \n",
       "3  657c10997e4f399c        33618   \n",
       "4  029ba12e494a33ff        33618   \n",
       "\n",
       "                                            category supercategory  \\\n",
       "0  http://commons.wikimedia.org/wiki/Category:Don...     sculpture   \n",
       "1  http://commons.wikimedia.org/wiki/Category:Don...     sculpture   \n",
       "2  http://commons.wikimedia.org/wiki/Category:Don...     sculpture   \n",
       "3  http://commons.wikimedia.org/wiki/Category:Don...     sculpture   \n",
       "4  http://commons.wikimedia.org/wiki/Category:Don...     sculpture   \n",
       "\n",
       "  hierarchical_label natural_or_human_made cleaned_supercategory  \\\n",
       "0          sculpture            human-made             sculpture   \n",
       "1          sculpture            human-made             sculpture   \n",
       "2          sculpture            human-made             sculpture   \n",
       "3          sculpture            human-made             sculpture   \n",
       "4          sculpture            human-made             sculpture   \n",
       "\n",
       "                                          image_path landmark_name  \n",
       "0  /kaggle/input/google-landmarks-v2-index-set-p3...  Dona i Ocell  \n",
       "1  /kaggle/input/google-landmarks-v2-index-set-p4...  Dona i Ocell  \n",
       "2  /kaggle/input/google-landmarks-v2-index-set-p1...  Dona i Ocell  \n",
       "3  /kaggle/input/google-landmarks-v2-index-set-p2...  Dona i Ocell  \n",
       "4  /kaggle/input/google-landmarks-v2-index-set-p1...  Dona i Ocell  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/google-landmarks-v2-index-subset-metadata/cleaned_landmark_index_subset_150k_v2.csv\")\n",
    "if DEBUG:\n",
    "    # Reduce to 1000 samples for quick debugging\n",
    "    train_df = train_df.sample(1000, random_state=42).reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0abe7f11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:00:38.306121Z",
     "iopub.status.busy": "2024-11-19T19:00:38.305861Z",
     "iopub.status.idle": "2024-11-19T19:00:38.331466Z",
     "shell.execute_reply": "2024-11-19T19:00:38.330872Z"
    },
    "papermill": {
     "duration": 0.033481,
     "end_time": "2024-11-19T19:00:38.333067",
     "exception": false,
     "start_time": "2024-11-19T19:00:38.299586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_CLASSES = train_df.landmark_name.nunique()\n",
    "class_names = train_df.landmark_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41661779",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:00:38.345084Z",
     "iopub.status.busy": "2024-11-19T19:00:38.344855Z",
     "iopub.status.idle": "2024-11-19T19:08:38.811104Z",
     "shell.execute_reply": "2024-11-19T19:08:38.810216Z"
    },
    "papermill": {
     "duration": 480.474884,
     "end_time": "2024-11-19T19:08:38.813403",
     "exception": false,
     "start_time": "2024-11-19T19:00:38.338519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 278M/278M [00:03<00:00, 77.3MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model\n",
    "embedding_device = \"cpu\"\n",
    "model, _ = clip.load(CLIP_MODEL_NAME, device=embedding_device)\n",
    "\n",
    "# Define class names and tokenize\n",
    "text_prompts = [f\"A photo of {name} or its part\" for name in class_names]\n",
    "text_tokens = clip.tokenize(text_prompts).to(embedding_device)\n",
    "\n",
    "# Compute text embeddings\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model.encode_text(text_tokens)\n",
    "text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)  # Normalize for cosine similarity\n",
    "\n",
    "# Save the embeddings\n",
    "torch.save(text_embeddings, \"text_embeddings.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9247485b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:08:38.830131Z",
     "iopub.status.busy": "2024-11-19T19:08:38.829855Z",
     "iopub.status.idle": "2024-11-19T19:08:38.871896Z",
     "shell.execute_reply": "2024-11-19T19:08:38.871091Z"
    },
    "papermill": {
     "duration": 0.051965,
     "end_time": "2024-11-19T19:08:38.873585",
     "exception": false,
     "start_time": "2024-11-19T19:08:38.821620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(text_embeddings, \"/kaggle/working/text_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "855b22a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:08:38.889132Z",
     "iopub.status.busy": "2024-11-19T19:08:38.888864Z",
     "iopub.status.idle": "2024-11-19T19:08:38.894794Z",
     "shell.execute_reply": "2024-11-19T19:08:38.894078Z"
    },
    "papermill": {
     "duration": 0.015446,
     "end_time": "2024-11-19T19:08:38.896355",
     "exception": false,
     "start_time": "2024-11-19T19:08:38.880909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_name_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "idx_to_class_name = {idx: class_name for class_name, idx in class_name_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc376250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:08:38.911508Z",
     "iopub.status.busy": "2024-11-19T19:08:38.911233Z",
     "iopub.status.idle": "2024-11-19T19:08:38.934025Z",
     "shell.execute_reply": "2024-11-19T19:08:38.932843Z"
    },
    "papermill": {
     "duration": 0.032229,
     "end_time": "2024-11-19T19:08:38.935792",
     "exception": false,
     "start_time": "2024-11-19T19:08:38.903563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pydantic/main.py:212: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    }
   ],
   "source": [
    "transforms_train = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=AUG_PROB),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=5),\n",
    "        A.MedianBlur(blur_limit=5),\n",
    "        A.GaussianBlur(blur_limit=5),\n",
    "        A.GaussNoise(var_limit=(5.0, 30.0)),\n",
    "    ], p=AUG_PROB),\n",
    "\n",
    "    A.OneOf([\n",
    "        A.OpticalDistortion(distort_limit=1.0),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=1.),\n",
    "        A.ElasticTransform(alpha=3),\n",
    "    ], p=AUG_PROB),\n",
    "\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=AUG_PROB),\n",
    "    A.Resize(IMG_SIZE[0], IMG_SIZE[1]),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, min_holes=1, min_height=8, min_width=8, p=AUG_PROB),    \n",
    "    A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], \n",
    "                         std=[0.26862954, 0.26130258, 0.27577711])\n",
    "])\n",
    "\n",
    "transforms_val = A.Compose([\n",
    "    A.Resize(IMG_SIZE[0], IMG_SIZE[1]),\n",
    "    A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], \n",
    "                         std=[0.26862954, 0.26130258, 0.27577711])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4f2ce09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:08:38.951927Z",
     "iopub.status.busy": "2024-11-19T19:08:38.951630Z",
     "iopub.status.idle": "2024-11-19T19:08:38.959016Z",
     "shell.execute_reply": "2024-11-19T19:08:38.958254Z"
    },
    "papermill": {
     "duration": 0.017668,
     "end_time": "2024-11-19T19:08:38.960702",
     "exception": false,
     "start_time": "2024-11-19T19:08:38.943034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GoogleLandmarksDataset(Dataset):\n",
    "    def __init__(self, df, class_name_to_idx, transform=None):\n",
    "        self.df = df\n",
    "        self.class_name_to_idx = class_name_to_idx  # Maps class name to index\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.df.iloc[idx]\n",
    "        class_name = t['landmark_name']\n",
    "        class_name_idx = self.class_name_to_idx[class_name]\n",
    "        \n",
    "        p = f'{t[\"image_path\"]}'\n",
    "        img = Image.open(p)\n",
    "        img = np.array(img)\n",
    "            \n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            img = self.transform(image=np.array(img))['image']\n",
    "        \n",
    "        # Convert image to tensor if needed\n",
    "        if not isinstance(img, torch.Tensor):\n",
    "            img = torch.tensor(img.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        \n",
    "        return img, class_name_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "165f0c94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:08:38.977315Z",
     "iopub.status.busy": "2024-11-19T19:08:38.976640Z",
     "iopub.status.idle": "2024-11-19T19:08:38.987443Z",
     "shell.execute_reply": "2024-11-19T19:08:38.986650Z"
    },
    "papermill": {
     "duration": 0.020832,
     "end_time": "2024-11-19T19:08:38.989065",
     "exception": false,
     "start_time": "2024-11-19T19:08:38.968233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrossModalDistillationModel(nn.Module):\n",
    "    def __init__(self, teacher_model, student_model, text_embeddings, projection_dim=512):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher_model\n",
    "        self.student = student_model\n",
    "        self.text_embeddings = text_embeddings\n",
    "        \n",
    "        # Freeze teacher\n",
    "        for param in self.teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Get input features dimension\n",
    "        in_features = student_model.get_classifier().in_features\n",
    "        \n",
    "        # Single projection head for student\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, projection_dim),\n",
    "            nn.BatchNorm1d(projection_dim)  # Added BatchNorm for stable training\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(in_features, student_model.get_classifier().out_features)\n",
    "        \n",
    "        # Temperature parameter\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        \n",
    "    def forward(self, images, return_features=True):\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Teacher forward pass\n",
    "        with torch.no_grad():\n",
    "            teacher_features = self.teacher.encode_image(images)\n",
    "            teacher_features = F.normalize(teacher_features, dim=-1)\n",
    "            teacher_features = teacher_features.float()\n",
    "            self.text_embeddings = self.text_embeddings.float()\n",
    "            teacher_logits = torch.matmul(teacher_features, self.text_embeddings.t())\n",
    "            teacher_logits = teacher_logits * self.temperature.exp()\n",
    "\n",
    "        # Student forward pass\n",
    "        student_features = self.student.forward_features(images)\n",
    "        student_features = F.adaptive_avg_pool2d(student_features, (1, 1))\n",
    "        student_features = student_features.view(batch_size, -1)\n",
    "\n",
    "        # Project student features\n",
    "        student_projected = self.projection_head(student_features)\n",
    "        student_projected = F.normalize(student_projected, dim=-1)\n",
    "\n",
    "        # Get classification logits\n",
    "        student_logits = self.classifier(student_features)\n",
    "\n",
    "        if return_features:\n",
    "            return {\n",
    "                'teacher_logits': teacher_logits,\n",
    "                'student_logits': student_logits,\n",
    "                'teacher_features': teacher_features,\n",
    "                'student_projected': student_projected,\n",
    "                'student_features': student_features\n",
    "            }\n",
    "        \n",
    "        return student_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25eb08d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:08:39.004178Z",
     "iopub.status.busy": "2024-11-19T19:08:39.003713Z",
     "iopub.status.idle": "2024-11-19T19:08:39.008278Z",
     "shell.execute_reply": "2024-11-19T19:08:39.007629Z"
    },
    "papermill": {
     "duration": 0.013754,
     "end_time": "2024-11-19T19:08:39.009792",
     "exception": false,
     "start_time": "2024-11-19T19:08:38.996038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.alpha = 0.4\n",
    "        self.beta = 0.3\n",
    "        self.gamma = 0.2\n",
    "        self.temperature = 2.0\n",
    "        self.device = device\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.n_workers = N_WORKERS\n",
    "        self.max_grad_norm = MAX_GRAD_NORM\n",
    "        self.log_interval = 100\n",
    "        self.use_amp = False\n",
    "        self.use_wandb = False  # Set to True if using wandb\n",
    "        self.warmup_steps = 100\n",
    "        self.gradient_accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11133223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:08:39.025199Z",
     "iopub.status.busy": "2024-11-19T19:08:39.024938Z",
     "iopub.status.idle": "2024-11-19T19:08:39.033284Z",
     "shell.execute_reply": "2024-11-19T19:08:39.032624Z"
    },
    "papermill": {
     "duration": 0.018117,
     "end_time": "2024-11-19T19:08:39.034859",
     "exception": false,
     "start_time": "2024-11-19T19:08:39.016742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class EnhancedDistillationLoss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.alpha = config.alpha  # KD loss weight\n",
    "        self.beta = config.beta    # Feature alignment weight\n",
    "        self.gamma = config.gamma  # CE loss weight\n",
    "        self.T = config.temperature\n",
    "        self.contrastive_weight = 0.1  # Weight for contrastive loss\n",
    "        \n",
    "    def contrastive_loss(self, features, labels):\n",
    "        # Implement contrastive loss between positive pairs\n",
    "        similarity = torch.matmul(features, features.t())\n",
    "        mask = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "        \n",
    "        # Temperature-scaled similarities\n",
    "        similarity = similarity / self.T\n",
    "        \n",
    "        # Positive and negative pairs\n",
    "        exp_sim = torch.exp(similarity)\n",
    "        pos_sim = exp_sim * mask\n",
    "        neg_sim = exp_sim * (1 - mask)\n",
    "        \n",
    "        loss = -torch.log(\n",
    "            pos_sim.sum(1) / (pos_sim.sum(1) + neg_sim.sum(1))\n",
    "        ).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        teacher_logits = outputs['teacher_logits']\n",
    "        student_logits = outputs['student_logits']\n",
    "        teacher_features = outputs['teacher_features']\n",
    "        student_projected = outputs['student_projected']\n",
    "        \n",
    "        # 1. Knowledge Distillation Loss\n",
    "        kd_loss = F.kl_div(\n",
    "            F.log_softmax(student_logits / self.T, dim=1),\n",
    "            F.softmax(teacher_logits / self.T, dim=1),\n",
    "            reduction=\"batchmean\"\n",
    "        ) * (self.T ** 2)\n",
    "        \n",
    "        # 2. Feature Alignment Loss\n",
    "        feature_loss = F.mse_loss(student_projected, teacher_features)\n",
    "        \n",
    "        # 3. Cross Entropy Loss\n",
    "        ce_loss = F.cross_entropy(student_logits, labels)\n",
    "        \n",
    "        # 4. Contrastive Loss\n",
    "        cont_loss = self.contrastive_loss(student_projected, labels)\n",
    "        \n",
    "        total_loss = (\n",
    "            self.alpha * kd_loss +\n",
    "            self.beta * feature_loss +\n",
    "            self.gamma * ce_loss +\n",
    "            self.contrastive_weight * cont_loss\n",
    "        )\n",
    "        \n",
    "        return total_loss, {\n",
    "            'kd_loss': kd_loss.item(),\n",
    "            'feature_loss': feature_loss.item(),\n",
    "            'ce_loss': ce_loss.item(),\n",
    "            'cont_loss': cont_loss.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27238d08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:08:39.050436Z",
     "iopub.status.busy": "2024-11-19T19:08:39.049801Z",
     "iopub.status.idle": "2024-11-19T19:08:39.057535Z",
     "shell.execute_reply": "2024-11-19T19:08:39.056781Z"
    },
    "papermill": {
     "duration": 0.017167,
     "end_time": "2024-11-19T19:08:39.059133",
     "exception": false,
     "start_time": "2024-11-19T19:08:39.041966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, config, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    metrics = defaultdict(float)\n",
    "    scaler = GradScaler() if config.use_amp else None\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Training')\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images = images.to(config.device)\n",
    "        labels = labels.to(config.device)\n",
    "        \n",
    "        # Mixed precision training\n",
    "        if config.use_amp:\n",
    "            with autocast():\n",
    "                outputs = model(images, return_features=True)\n",
    "                loss, batch_metrics = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if config.max_grad_norm:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images, return_features=True)\n",
    "            loss, batch_metrics = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            if config.max_grad_norm:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_loss += loss.item()\n",
    "        for k, v in batch_metrics.items():\n",
    "            metrics[k] += v\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'avg_loss': f'{total_loss/(batch_idx+1):.4f}'\n",
    "        })\n",
    "        \n",
    "    metrics = {k: v/len(train_loader) for k, v in metrics.items()}\n",
    "    return total_loss / len(train_loader), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12e50291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:08:39.074207Z",
     "iopub.status.busy": "2024-11-19T19:08:39.073962Z",
     "iopub.status.idle": "2024-11-19T19:08:39.077415Z",
     "shell.execute_reply": "2024-11-19T19:08:39.076639Z"
    },
    "papermill": {
     "duration": 0.012891,
     "end_time": "2024-11-19T19:08:39.079093",
     "exception": false,
     "start_time": "2024-11-19T19:08:39.066202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d1c3e81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:08:39.094497Z",
     "iopub.status.busy": "2024-11-19T19:08:39.094273Z",
     "iopub.status.idle": "2024-11-19T19:08:39.098566Z",
     "shell.execute_reply": "2024-11-19T19:08:39.097776Z"
    },
    "papermill": {
     "duration": 0.013971,
     "end_time": "2024-11-19T19:08:39.100368",
     "exception": false,
     "start_time": "2024-11-19T19:08:39.086397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = GoogleLandmarksDataset(train_df, class_name_to_idx=class_name_to_idx, transform=transforms_train)\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    num_workers=N_WORKERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "317cd2f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T19:08:39.116155Z",
     "iopub.status.busy": "2024-11-19T19:08:39.115913Z",
     "iopub.status.idle": "2024-11-19T19:27:58.698357Z",
     "shell.execute_reply": "2024-11-19T19:27:58.697497Z"
    },
    "papermill": {
     "duration": 1159.593129,
     "end_time": "2024-11-19T19:27:58.700542",
     "exception": false,
     "start_time": "2024-11-19T19:08:39.107413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5994050a2ae844be895e0140217804ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2344/2344 [19:13<00:00,  2.03it/s, loss=2.0843, avg_loss=2.1292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]\n",
      "Train Loss: 2.1292\n",
      "kd_loss: 0.1469\n",
      "feature_loss: 0.0034\n",
      "ce_loss: 8.5381\n",
      "cont_loss: 3.6172\n"
     ]
    }
   ],
   "source": [
    "teacher_model, preprocess = clip.load(CLIP_MODEL_NAME, device=device)\n",
    "student_model = timm.create_model(\"resnet18\", pretrained=True, num_classes=N_CLASSES)\n",
    "\n",
    "# Initialize the improved distillation model\n",
    "distillation_model = CrossModalDistillationModel(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    text_embeddings=text_embeddings.to(device),\n",
    "    projection_dim=512  # Same dimension as CLIP's feature space\n",
    ")\n",
    "distillation_model.to(device)\n",
    "\n",
    "# Optimizer with weight decay\n",
    "optimizer = AdamW([\n",
    "    {'params': distillation_model.student.parameters(), 'lr': 1e-3},\n",
    "    {'params': distillation_model.projection_head.parameters(), 'lr': 1e-3}\n",
    "], weight_decay=0.01)\n",
    "\n",
    "# Scheduler with warmup\n",
    "num_training_steps = len(train_dl) * EPOCHS\n",
    "num_warmup_steps = min(1000, num_training_steps // 10)  # 10% of training steps or 1000, whichever is smaller\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Training loop with validation\n",
    "best_loss = float('inf')\n",
    "patience = EARLY_STOPPING_EPOCH\n",
    "patience_counter = 0\n",
    "\n",
    "scaler = GradScaler() if USE_AMP else None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    train_loss, train_metrics = train_epoch(\n",
    "        model=distillation_model,\n",
    "        train_loader=train_dl,\n",
    "        criterion=EnhancedDistillationLoss(Config()),\n",
    "        optimizer=optimizer,\n",
    "        config=Config(),\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}]')\n",
    "    print(f'Train Loss: {train_loss:.4f}')\n",
    "    for metric_name, metric_value in train_metrics.items():\n",
    "        print(f'{metric_name}: {metric_value:.4f}')\n",
    "    \n",
    "    \n",
    "    # Early stopping\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': distillation_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': train_loss,\n",
    "        }, f'{OUTPUT_DIR}/best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered after epoch {epoch+1}')\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5883095,
     "sourceId": 9635661,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5883097,
     "sourceId": 9635664,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5883577,
     "sourceId": 9636294,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5883578,
     "sourceId": 9636295,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5883713,
     "sourceId": 9636477,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5882766,
     "sourceId": 9720780,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5935937,
     "sourceId": 9828230,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 204129443,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1681.458035,
   "end_time": "2024-11-19T19:28:01.479744",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-19T19:00:00.021709",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "168a7d35f6a040b3a3aae3cdbe2734ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "24520c242e914dfb8510770937925ab9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_432b70c74ed44a47b0b620d473592686",
       "max": 46807446.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_baba491d2e774dd0bc2872eb8a0e409c",
       "value": 46807446.0
      }
     },
     "2f64b92e6ba24ec1ba604f005ec619b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3e6e69cd78fa4faf902f57bbc4e09eee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "432b70c74ed44a47b0b620d473592686": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5994050a2ae844be895e0140217804ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fc002324db924a58a008fcf20e1673a1",
        "IPY_MODEL_24520c242e914dfb8510770937925ab9",
        "IPY_MODEL_cd9e6b3903d1445695f84daa72df41b7"
       ],
       "layout": "IPY_MODEL_2f64b92e6ba24ec1ba604f005ec619b6"
      }
     },
     "97def501c9ad4ad6844f77d3851805d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "baba491d2e774dd0bc2872eb8a0e409c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cd9e6b3903d1445695f84daa72df41b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3e6e69cd78fa4faf902f57bbc4e09eee",
       "placeholder": "​",
       "style": "IPY_MODEL_168a7d35f6a040b3a3aae3cdbe2734ae",
       "value": " 46.8M/46.8M [00:00&lt;00:00, 185MB/s]"
      }
     },
     "e0a11eea8d764e1bbf33f920dd096a05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fc002324db924a58a008fcf20e1673a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e0a11eea8d764e1bbf33f920dd096a05",
       "placeholder": "​",
       "style": "IPY_MODEL_97def501c9ad4ad6844f77d3851805d4",
       "value": "model.safetensors: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
